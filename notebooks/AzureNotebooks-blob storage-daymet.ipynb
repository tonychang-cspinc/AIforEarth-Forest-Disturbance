{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo notebook for accessing Daymet data on Azure\n",
    "\n",
    "The Daymet dataset contains daily minimum temperature, maximum temperature, precipitation, shortwave radiation, vapor pressure, snow water equivalent, and day length at 1km resolution for North America. The dataset covers the period from January 1, 1980 to December 31, 2019.  \n",
    "\n",
    "The Daymet dataset is maintained at [daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1328](https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1328) and mirrored on Azure Open Datasets at [azure.microsoft.com/services/open-datasets/catalog/daymet](https://azure.microsoft.com/services/open-datasets/catalog/daymet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard or standard-ish imports\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "#import cartopy.crs as ccrs\n",
    "import urllib.request\n",
    "\n",
    "# Less standard, but still pip- or conda-installable\n",
    "from azure.storage.blob import ContainerClient\n",
    "\n",
    "container_name = 'daymetv3-raw'\n",
    "daymet_azure_storage_url = 'https://daymet.blob.core.windows.net/'\n",
    "\n",
    "daymet_container_client = ContainerClient(account_url=daymet_azure_storage_url, \n",
    "                                         container_name=container_name,\n",
    "                                         credential=None)\n",
    "\n",
    "# Temporary folder for data we need during execution of this notebook (we'll clean up\n",
    "# at the end, we promise)\n",
    "temp_dir = os.path.join('/home/cspadmin','daymet')\n",
    "os.makedirs(temp_dir,exist_ok=True)\n",
    "os.listdir(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url, destination_filename=None, progress_updater=None, force_download=False):\n",
    "    \"\"\"\n",
    "    Download a URL to a temporary file\n",
    "    \"\"\"\n",
    "    \n",
    "    # This is not intended to guarantee uniqueness, we just know it happens to guarantee\n",
    "    # uniqueness for this application.\n",
    "    if destination_filename is None:\n",
    "        url_as_filename = url.replace('://', '_').replace('.', '_').replace('/', '_')\n",
    "        destination_filename = \\\n",
    "            os.path.join(temp_dir,url_as_filename)\n",
    "    if (not force_download) and (os.path.isfile(destination_filename)):\n",
    "        print('Bypassing download of already-downloaded file {}'.format(os.path.basename(url)))\n",
    "        return destination_filename\n",
    "    print('Downloading file {}'.format(os.path.basename(url)),end='')\n",
    "    urllib.request.urlretrieve(url, destination_filename, progress_updater)  \n",
    "    assert(os.path.isfile(destination_filename))\n",
    "    nBytes = os.path.getsize(destination_filename)\n",
    "    print('...done, {} bytes.'.format(nBytes))\n",
    "    return destination_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a specific file from Azure blob storage\n",
    "This code shows how to download a specific file from Azure blob storage into the current directory.  It uses the example file daymet_v3_tmax_1980_hawaii.nc4, but you can change this as described below.  \n",
    "\n",
    "The following types of data are available: minimum temperature (tmin), maximum temperature (tmax), precipitation (prcp), shortwave radiation (srad), vapor pressure (vp), snow water equivalent (swe), and day length (dayl).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filename(my_variable, year, location):\n",
    "    destination_file = my_variable + '_' + str(year) + '_' + location\n",
    "    granule_name = 'daymet_v3_' + destination_file + '.nc4'\n",
    "    url = 'https://daymet.blob.core.windows.net/daymetv3-raw/' + granule_name\n",
    "    return download_url(url, destination_filename=os.path.join(temp_dir, destination_file + '.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the NetCDF metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Region:\n",
    "    def __init__(self, name, ds, df, var):\n",
    "        self.name = name\n",
    "        self.df = df\n",
    "        self.ds = ds\n",
    "        self.var = var\n",
    "        self.calculate_reduction2()\n",
    "        self.avg_sum()\n",
    "        #self.calculate_std()\n",
    "        #self.write_daymet()\n",
    "        #self.get_points()\n",
    "        #self.concat()\n",
    "        #self.fix_df()\n",
    "        \n",
    "        \n",
    "    def setup_original_df(self):\n",
    "        self.xy = [data_crs.transform_point(x, y, src_crs=ccrs.PlateCarree()) for x, y in zip(self.df['LON'], self.df['LAT'])]\n",
    "        self.df['x_exact'], self.df['y_exact'] = list(zip(*self.xy))\n",
    "        self.df.reset_index(inplace=True)\n",
    "        \n",
    "    def setup_unique_df(self):\n",
    "        self.unique = self.df[['newlat', 'newlon', 'latlon_id']].drop_duplicates()\n",
    "        self.xy = [data_crs.transform_point(x, y, src_crs=ccrs.PlateCarree()) for x, y in zip(self.unique['newlon'], self.unique['newlat'])]\n",
    "        self.xy_round = [(int(np.around(x/250, decimals=0)*250), int(np.around(y/250, decimals=0)*250)) for x, y in self.xy]\n",
    "        self.unique['x_exact'], self.unique['y_exact'] = list(zip(*self.xy_round))\n",
    "        \n",
    "    def get_points(self):\n",
    "        self.points = [Point(self, self.df['y_exact'].iloc[row], self.df['x_exact'].iloc[row]) for row in range(len(self.df))]\n",
    "               \n",
    "    def fix_df(self):\n",
    "        self.df['newlat'] = [r.point.coords['lat'].values for r in self.points]\n",
    "        self.df['newlon'] = [r.point.coords['lon'].values for r in self.points]\n",
    "        self.df['latlon_id'] = make_identifier(self.df[['newlat', 'newlon']])\n",
    "        self.df.drop(['x_exact', 'y_exact'], axis=1, inplace=True)\n",
    "        self.df.to_csv(self.name + '_nearest.csv', index=False)\n",
    "    \n",
    "    def concat(self):\n",
    "        self.set = xr.concat([r.point for r in self.points], dim=\"locs\")\n",
    "        \n",
    "    def find_original_xy_indices(self):\n",
    "        self.unique['x_ind'] = [np.where(self.ds.indexes['x'] == num)[0][0] for num in self.unique['x_exact']]\n",
    "        self.unique['y_ind'] = [np.where(self.ds.indexes['y'] == num)[0][0] for num in self.unique['y_exact']]\n",
    "        \n",
    "    def calculate_reduction2(self):\n",
    "        self.time_value_gen = (self.ds[self.var].isel(time=my_time).values for my_time in range(365))\n",
    "        self.reduction = next(self.time_value_gen)\n",
    "        for i in range(1,365):\n",
    "            self.reduction += next(self.time_value_gen)\n",
    "        self.reduction /= 365.\n",
    "        \n",
    "    def avg_sum(self):\n",
    "        self.avg = [self.reduction[y,x] for x, y in zip(self.df['x_ind'], self.df['y_ind'])]\n",
    "        \n",
    "    def calculate_reduction(self):\n",
    "        self.reduction = self.ds.groupby('time.year').sum(dim='time').mean(dim='year')\n",
    "        self.avg_sum = np.empty(len(self.unique))\n",
    "        for idx, itr in enumerate(range(int(np.floor(len(self.unique)/1000)))):\n",
    "            self.avg_sum[itr*1000:(itr+1)*1000] = np.diagonal(self.reduction.sel(y=self.unique['y_exact'][itr*1000:(itr+1)*1000], x=self.unique['x_exact'][itr*1000:(itr+1)*1000])['prcp'].values)\n",
    "            print(idx)\n",
    "        self.unique['prcp_avg'] = self.avg_sum\n",
    "        \n",
    "    def calculate_std(self):\n",
    "        self.std = self.ds.groupby('time.year').std(dim='time').mean(dim='year')\n",
    "        self.avg_std = np.empty(len(self.unique))\n",
    "        for idx, itr in enumerate(range(int(np.floor(len(self.unique)/1000)))):\n",
    "            self.avg_std[itr*1000:(itr+1)*1000] = np.diagonal(self.std.sel(y=self.unique['y_exact'][itr*1000:(itr+1)*1000], x=self.unique['x_exact'][itr*1000:(itr+1)*1000])['prcp'].values)\n",
    "            print(idx)\n",
    "        self.unique['prcp_std'] = self.avg_std\n",
    "        \n",
    "    def write_daymet(self):\n",
    "        self.df.to_csv(self.name + '_daymet.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = \"tmax\"\n",
    "#for region in [\"hawaii\", \"puertorico\", \"na\"]:\n",
    "for region in [\"hawaii\"]:\n",
    "    print(region)\n",
    "#    df_region = pd.read_csv(region + '_daymet.csv')\n",
    "    my_files = [make_filename(variable, 1980, region)]\n",
    "#    ds_region = xr.open_dataset(my_basefile)\n",
    "#    base_region = Region(region, ds_region, df_region, variable)\n",
    "#    region_var = base_region.avg\n",
    "    for i in range(1981,2011):\n",
    "        print(i)\n",
    "        my_files.append(make_filename(variable, i, region))\n",
    " #       my_region = xr.open_dataset(my_file)\n",
    " #       ds_region_new = xr.concat([ds_region, my_region], dim='time')\n",
    " #       del ds_region\n",
    " #       del my_region\n",
    " #       ds_region = ds_region_new\n",
    " #       del ds_region_new\n",
    "    ds_region = xr.open_mfdataset(my_files[:-1])\n",
    "    ds_region.drop_vars('lambert_conformal_conic').to_zarr('/home/datablob/daymet/hi-zarr-full')\n",
    "#        region_var += np.array(my_region.avg)\n",
    "#        os.remove(my_file)\n",
    "#        del my_region\n",
    "#    region_var /= 31.\n",
    "#    base_region.df[variable + '_avg'] = region_var\n",
    "#    del region_var\n",
    "#    base_region.write_daymet()\n",
    "#    os.remove(my_basefile)\n",
    "#    del base_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_zarr = xr.open_dataset(my_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_zarr.drop_vars('lambert_conformal_conic').to_zarr('/home/datablob/daymet/hi-zarr-full', append_dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_zarr3 = xr.open_zarr('/home/datablob/daymet/hi-zarr-full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_zarr3['tmax'].groupby('time.year').mean(dim='time').isel(year=9).isel(x=23, y=24).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fia_no_pltcn.csv')\n",
    "df_hi = df.loc[df['LAT'] < 25].loc[df['LON'] < -100]\n",
    "df_pr = df.loc[df['LAT'] < 20].loc[df['LON'] > -70]\n",
    "df_na = df[~df.isin(df_hi)][~df.isin(df_pr)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crs = ccrs.LambertConformal(central_longitude=-100, central_latitude=42.5, standard_parallels=(25, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point:\n",
    "    def __init__(self, parent, y, x):\n",
    "        self.parent = parent\n",
    "        self.y = y\n",
    "        self.x = x\n",
    "        self.index = {}\n",
    "        self.original_point = self.parent.ds.sel(y=self.y, method='nearest').sel(x=self.x, method='nearest')\n",
    "        self.find_original_xy_indices()\n",
    "        self.need_recalculate()\n",
    "        self.get_new_point()\n",
    "        \n",
    "    def find_original_xy_indices(self):\n",
    "        self.index['x'] = np.where(self.parent.ds.indexes['x'] == self.original_point.coords['x'].values)[0][0]\n",
    "        self.index['y'] = np.where(self.parent.ds.indexes['y'] == self.original_point.coords['y'].values)[0][0]\n",
    "        \n",
    "    def need_recalculate(self):\n",
    "        if np.isnan(self.original_point['prcp'].isel(time=0)):\n",
    "            self.find_nonnan()\n",
    "        else:\n",
    "            self.indices = self.index['x'], self.index['y']\n",
    "            \n",
    "    def find_nonnan(self):\n",
    "        my_x = self.index['x']\n",
    "        my_y = self.index['y']\n",
    "        for j in [my_y, my_y-1, my_y+1, my_y-2, my_y+2, my_y-3, my_y+3]:\n",
    "            for i in [my_x, my_x-1, my_x+1, my_x-2, my_x+2, my_x-3, my_x+3]:\n",
    "                if i==my_x and j==my_y:\n",
    "                    continue\n",
    "                if i>=self.parent.ds.indexes['x'].size or i<=-1:\n",
    "                    continue\n",
    "                if j>=self.parent.ds.indexes['y'].size or j<=-1:\n",
    "                    continue\n",
    "                if not np.isnan(self.parent.ds['prcp'].isel(time=0).isel(x=i).isel(y=j)):\n",
    "                    self.indices = i, j\n",
    "        self.indices = -1, -1\n",
    "        \n",
    "    def get_new_point(self):\n",
    "        self.point = self.parent.ds.isel(x=self.indices[0]).isel(y=self.indices[1])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_identifier(df):\n",
    "    str_id = df.apply(lambda x: '_'.join(map(str, x)), axis=1)\n",
    "    return pd.factorize(str_id)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!conda install -y -c conda-forge h5netcdf xarray cartopy\n",
    "!{sys.executable} -m pip install azure.storage.blob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
