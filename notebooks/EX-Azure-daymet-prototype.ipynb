{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo notebook for accessing Daymet data on Azure\n",
    "\n",
    "The Daymet dataset contains daily minimum temperature, maximum temperature, precipitation, shortwave radiation, vapor pressure, snow water equivalent, and day length at 1km resolution for North America. The dataset covers the period from January 1, 1980 to December 31, 2019.  \n",
    "\n",
    "The Daymet dataset is maintained at [daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1328](https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1328) and mirrored on Azure Open Datasets at [azure.microsoft.com/services/open-datasets/catalog/daymet](https://azure.microsoft.com/services/open-datasets/catalog/daymet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard or standard-ish imports\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import urllib.request\n",
    "\n",
    "# Less standard, but still pip- or conda-installable\n",
    "from azure.storage.blob import ContainerClient\n",
    "\n",
    "container_name = 'daymetv3-raw'\n",
    "daymet_azure_storage_url = 'https://daymet.blob.core.windows.net/'\n",
    "\n",
    "daymet_container_client = ContainerClient(account_url=daymet_azure_storage_url, \n",
    "                                         container_name=container_name,\n",
    "                                         credential=None)\n",
    "\n",
    "# Temporary folder for data we need during execution of this notebook (we'll clean up\n",
    "# at the end, we promise)\n",
    "temp_dir = os.path.join('/home/cspadmin','daymet')\n",
    "os.makedirs(temp_dir,exist_ok=True)\n",
    "os.listdir(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url, destination_filename=None, progress_updater=None, force_download=False):\n",
    "    \"\"\"\n",
    "    Download a URL to a temporary file\n",
    "    \"\"\"\n",
    "    \n",
    "    # This is not intended to guarantee uniqueness, we just know it happens to guarantee\n",
    "    # uniqueness for this application.\n",
    "    if destination_filename is None:\n",
    "        url_as_filename = url.replace('://', '_').replace('.', '_').replace('/', '_')\n",
    "        destination_filename = \\\n",
    "            os.path.join(temp_dir,url_as_filename)\n",
    "    if (not force_download) and (os.path.isfile(destination_filename)):\n",
    "        print('Bypassing download of already-downloaded file {}'.format(os.path.basename(url)))\n",
    "        return destination_filename\n",
    "    print('Downloading file {}'.format(os.path.basename(url)),end='')\n",
    "    urllib.request.urlretrieve(url, destination_filename, progress_updater)  \n",
    "    assert(os.path.isfile(destination_filename))\n",
    "    nBytes = os.path.getsize(destination_filename)\n",
    "    print('...done, {} bytes.'.format(nBytes))\n",
    "    return destination_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a specific file from Azure blob storage\n",
    "This code shows how to download a specific file from Azure blob storage into the current directory.  It uses the example file daymet_v3_tmax_1980_hawaii.nc4, but you can change this as described below.  \n",
    "\n",
    "The following types of data are available: minimum temperature (tmin), maximum temperature (tmax), precipitation (prcp), shortwave radiation (srad), vapor pressure (vp), snow water equivalent (swe), and day length (dayl).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filename(my_variable, year, location):\n",
    "    destination_file = my_variable + '_' + str(year) + '_' + location\n",
    "    granule_name = 'daymet_v3_' + destination_file + '.nc4'\n",
    "    url = 'https://daymet.blob.core.windows.net/daymetv3-raw/' + granule_name\n",
    "    return download_url(url, destination_filename=os.path.join(temp_dir, destination_file + '.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the NetCDF metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Region:\n",
    "    def __init__(self, name, ds, df, var):\n",
    "        self.name = name\n",
    "        self.df = df\n",
    "        self.ds = ds\n",
    "        self.var = var\n",
    "        self.calculate_reduction2()\n",
    "        self.avg_sum()\n",
    "        #self.calculate_std()\n",
    "        #self.write_daymet()\n",
    "        #self.get_points()\n",
    "        #self.concat()\n",
    "        #self.fix_df()\n",
    "        \n",
    "        \n",
    "    def setup_original_df(self):\n",
    "        self.xy = [data_crs.transform_point(x, y, src_crs=ccrs.PlateCarree()) for x, y in zip(self.df['LON'], self.df['LAT'])]\n",
    "        self.df['x_exact'], self.df['y_exact'] = list(zip(*self.xy))\n",
    "        self.df.reset_index(inplace=True)\n",
    "        \n",
    "    def setup_unique_df(self):\n",
    "        self.unique = self.df[['newlat', 'newlon', 'latlon_id']].drop_duplicates()\n",
    "        self.xy = [data_crs.transform_point(x, y, src_crs=ccrs.PlateCarree()) for x, y in zip(self.unique['newlon'], self.unique['newlat'])]\n",
    "        self.xy_round = [(int(np.around(x/250, decimals=0)*250), int(np.around(y/250, decimals=0)*250)) for x, y in self.xy]\n",
    "        self.unique['x_exact'], self.unique['y_exact'] = list(zip(*self.xy_round))\n",
    "        \n",
    "    def get_points(self):\n",
    "        self.points = [Point(self, self.df['y_exact'].iloc[row], self.df['x_exact'].iloc[row]) for row in range(len(self.df))]\n",
    "               \n",
    "    def fix_df(self):\n",
    "        self.df['newlat'] = [r.point.coords['lat'].values for r in self.points]\n",
    "        self.df['newlon'] = [r.point.coords['lon'].values for r in self.points]\n",
    "        self.df['latlon_id'] = make_identifier(self.df[['newlat', 'newlon']])\n",
    "        self.df.drop(['x_exact', 'y_exact'], axis=1, inplace=True)\n",
    "        self.df.to_csv(self.name + '_nearest.csv', index=False)\n",
    "    \n",
    "    def concat(self):\n",
    "        self.set = xr.concat([r.point for r in self.points], dim=\"locs\")\n",
    "        \n",
    "    def find_original_xy_indices(self):\n",
    "        self.unique['x_ind'] = [np.where(self.ds.indexes['x'] == num)[0][0] for num in self.unique['x_exact']]\n",
    "        self.unique['y_ind'] = [np.where(self.ds.indexes['y'] == num)[0][0] for num in self.unique['y_exact']]\n",
    "        \n",
    "    def calculate_reduction2(self):\n",
    "        self.time_value_gen = (self.ds[self.var].isel(time=my_time).values for my_time in range(365))\n",
    "        self.reduction = next(self.time_value_gen)\n",
    "        for i in range(1,365):\n",
    "            self.reduction += next(self.time_value_gen)\n",
    "        self.reduction /= 365.\n",
    "        \n",
    "    def avg_sum(self):\n",
    "        self.avg = [self.reduction[y,x] for x, y in zip(self.df['x_ind'], self.df['y_ind'])]\n",
    "        \n",
    "    def calculate_reduction(self):\n",
    "        self.reduction = self.ds.groupby('time.year').sum(dim='time').mean(dim='year')\n",
    "        self.avg_sum = np.empty(len(self.unique))\n",
    "        for idx, itr in enumerate(range(int(np.floor(len(self.unique)/1000)))):\n",
    "            self.avg_sum[itr*1000:(itr+1)*1000] = np.diagonal(self.reduction.sel(y=self.unique['y_exact'][itr*1000:(itr+1)*1000], x=self.unique['x_exact'][itr*1000:(itr+1)*1000])['prcp'].values)\n",
    "            print(idx)\n",
    "        self.unique['prcp_avg'] = self.avg_sum\n",
    "        \n",
    "    def calculate_std(self):\n",
    "        self.std = self.ds.groupby('time.year').std(dim='time').mean(dim='year')\n",
    "        self.avg_std = np.empty(len(self.unique))\n",
    "        for idx, itr in enumerate(range(int(np.floor(len(self.unique)/1000)))):\n",
    "            self.avg_std[itr*1000:(itr+1)*1000] = np.diagonal(self.std.sel(y=self.unique['y_exact'][itr*1000:(itr+1)*1000], x=self.unique['x_exact'][itr*1000:(itr+1)*1000])['prcp'].values)\n",
    "            print(idx)\n",
    "        self.unique['prcp_std'] = self.avg_std\n",
    "        \n",
    "    def write_daymet(self):\n",
    "        self.df.to_csv(self.name + '_daymet.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hawaii\n",
      "Downloading file daymet_v3_swe_1980_hawaii.nc4...done, 2113809 bytes.\n",
      "1981\n",
      "Downloading file daymet_v3_swe_1981_hawaii.nc4...done, 2098334 bytes.\n",
      "1982\n",
      "Downloading file daymet_v3_swe_1982_hawaii.nc4...done, 2122885 bytes.\n",
      "1983\n",
      "Downloading file daymet_v3_swe_1983_hawaii.nc4...done, 2101542 bytes.\n",
      "1984\n",
      "Downloading file daymet_v3_swe_1984_hawaii.nc4...done, 2125106 bytes.\n",
      "1985\n",
      "Downloading file daymet_v3_swe_1985_hawaii.nc4...done, 2130851 bytes.\n",
      "1986\n",
      "Downloading file daymet_v3_swe_1986_hawaii.nc4...done, 2096965 bytes.\n",
      "1987\n",
      "Downloading file daymet_v3_swe_1987_hawaii.nc4...done, 2134821 bytes.\n",
      "1988\n",
      "Downloading file daymet_v3_swe_1988_hawaii.nc4...done, 2115344 bytes.\n",
      "1989\n",
      "Downloading file daymet_v3_swe_1989_hawaii.nc4...done, 2187549 bytes.\n",
      "1990\n",
      "Downloading file daymet_v3_swe_1990_hawaii.nc4...done, 2171444 bytes.\n",
      "1991\n",
      "Downloading file daymet_v3_swe_1991_hawaii.nc4...done, 2112862 bytes.\n",
      "1992\n",
      "Downloading file daymet_v3_swe_1992_hawaii.nc4...done, 2129125 bytes.\n",
      "1993\n",
      "Downloading file daymet_v3_swe_1993_hawaii.nc4...done, 2243422 bytes.\n",
      "1994\n",
      "Downloading file daymet_v3_swe_1994_hawaii.nc4...done, 2176520 bytes.\n",
      "1995\n",
      "Downloading file daymet_v3_swe_1995_hawaii.nc4...done, 2123077 bytes.\n",
      "1996\n",
      "Downloading file daymet_v3_swe_1996_hawaii.nc4...done, 2134957 bytes.\n",
      "1997\n",
      "Downloading file daymet_v3_swe_1997_hawaii.nc4...done, 2249986 bytes.\n",
      "1998\n",
      "Downloading file daymet_v3_swe_1998_hawaii.nc4...done, 2123399 bytes.\n",
      "1999\n",
      "Downloading file daymet_v3_swe_1999_hawaii.nc4...done, 2204174 bytes.\n",
      "2000\n",
      "Downloading file daymet_v3_swe_2000_hawaii.nc4...done, 2129361 bytes.\n",
      "2001\n",
      "Downloading file daymet_v3_swe_2001_hawaii.nc4...done, 2103842 bytes.\n",
      "2002\n",
      "Downloading file daymet_v3_swe_2002_hawaii.nc4...done, 2194339 bytes.\n",
      "2003\n",
      "Downloading file daymet_v3_swe_2003_hawaii.nc4...done, 2141093 bytes.\n",
      "2004\n",
      "Downloading file daymet_v3_swe_2004_hawaii.nc4...done, 2165375 bytes.\n",
      "2005\n",
      "Downloading file daymet_v3_swe_2005_hawaii.nc4...done, 2147146 bytes.\n",
      "2006\n",
      "Downloading file daymet_v3_swe_2006_hawaii.nc4...done, 2115082 bytes.\n",
      "2007\n",
      "Downloading file daymet_v3_swe_2007_hawaii.nc4...done, 2104248 bytes.\n",
      "2008\n",
      "Downloading file daymet_v3_swe_2008_hawaii.nc4...done, 2105157 bytes.\n",
      "2009\n",
      "Downloading file daymet_v3_swe_2009_hawaii.nc4...done, 2127475 bytes.\n",
      "2010\n",
      "Downloading file daymet_v3_swe_2010_hawaii.nc4...done, 2097505 bytes.\n",
      "puertorico\n",
      "Downloading file daymet_v3_swe_1980_puertorico.nc4...done, 1341025 bytes.\n",
      "1981\n",
      "Downloading file daymet_v3_swe_1981_puertorico.nc4...done, 1341025 bytes.\n",
      "1982\n",
      "Downloading file daymet_v3_swe_1982_puertorico.nc4...done, 1341025 bytes.\n",
      "1983\n",
      "Downloading file daymet_v3_swe_1983_puertorico.nc4...done, 1341025 bytes.\n",
      "1984\n",
      "Downloading file daymet_v3_swe_1984_puertorico.nc4...done, 1341025 bytes.\n",
      "1985\n",
      "Downloading file daymet_v3_swe_1985_puertorico.nc4...done, 1341025 bytes.\n",
      "1986\n",
      "Downloading file daymet_v3_swe_1986_puertorico.nc4...done, 1341025 bytes.\n",
      "1987\n",
      "Downloading file daymet_v3_swe_1987_puertorico.nc4...done, 1341025 bytes.\n",
      "1988\n",
      "Downloading file daymet_v3_swe_1988_puertorico.nc4...done, 1341025 bytes.\n",
      "1989\n",
      "Downloading file daymet_v3_swe_1989_puertorico.nc4...done, 1341025 bytes.\n",
      "1990\n",
      "Downloading file daymet_v3_swe_1990_puertorico.nc4...done, 1341025 bytes.\n",
      "1991\n",
      "Downloading file daymet_v3_swe_1991_puertorico.nc4...done, 1341025 bytes.\n",
      "1992\n",
      "Downloading file daymet_v3_swe_1992_puertorico.nc4...done, 1341025 bytes.\n",
      "1993\n",
      "Downloading file daymet_v3_swe_1993_puertorico.nc4...done, 1341025 bytes.\n",
      "1994\n",
      "Downloading file daymet_v3_swe_1994_puertorico.nc4...done, 1341025 bytes.\n",
      "1995\n",
      "Downloading file daymet_v3_swe_1995_puertorico.nc4...done, 1341025 bytes.\n",
      "1996\n",
      "Downloading file daymet_v3_swe_1996_puertorico.nc4...done, 1341025 bytes.\n",
      "1997\n",
      "Downloading file daymet_v3_swe_1997_puertorico.nc4...done, 1341025 bytes.\n",
      "1998\n",
      "Downloading file daymet_v3_swe_1998_puertorico.nc4...done, 1341025 bytes.\n",
      "1999\n",
      "Downloading file daymet_v3_swe_1999_puertorico.nc4...done, 1341025 bytes.\n",
      "2000\n",
      "Downloading file daymet_v3_swe_2000_puertorico.nc4...done, 1341025 bytes.\n",
      "2001\n",
      "Downloading file daymet_v3_swe_2001_puertorico.nc4...done, 1341025 bytes.\n",
      "2002\n",
      "Downloading file daymet_v3_swe_2002_puertorico.nc4...done, 1341025 bytes.\n",
      "2003\n",
      "Downloading file daymet_v3_swe_2003_puertorico.nc4...done, 1341025 bytes.\n",
      "2004\n",
      "Downloading file daymet_v3_swe_2004_puertorico.nc4...done, 1341025 bytes.\n",
      "2005\n",
      "Downloading file daymet_v3_swe_2005_puertorico.nc4...done, 1341025 bytes.\n",
      "2006\n",
      "Downloading file daymet_v3_swe_2006_puertorico.nc4...done, 1341025 bytes.\n",
      "2007\n",
      "Downloading file daymet_v3_swe_2007_puertorico.nc4...done, 1341025 bytes.\n",
      "2008\n",
      "Downloading file daymet_v3_swe_2008_puertorico.nc4...done, 1341025 bytes.\n",
      "2009\n",
      "Downloading file daymet_v3_swe_2009_puertorico.nc4...done, 1341025 bytes.\n",
      "2010\n",
      "Downloading file daymet_v3_swe_2010_puertorico.nc4...done, 1341025 bytes.\n",
      "na\n",
      "Downloading file daymet_v3_swe_1980_na.nc4...done, 2685885622 bytes.\n",
      "1981\n",
      "Downloading file daymet_v3_swe_1981_na.nc4...done, 2606294975 bytes.\n",
      "1982\n",
      "Downloading file daymet_v3_swe_1982_na.nc4...done, 2961853834 bytes.\n",
      "1983\n",
      "Downloading file daymet_v3_swe_1983_na.nc4...done, 2761426884 bytes.\n",
      "1984\n",
      "Downloading file daymet_v3_swe_1984_na.nc4...done, 2825736428 bytes.\n",
      "1985\n",
      "Downloading file daymet_v3_swe_1985_na.nc4...done, 2932898085 bytes.\n",
      "1986\n",
      "Downloading file daymet_v3_swe_1986_na.nc4...done, 2666304459 bytes.\n",
      "1987\n",
      "Downloading file daymet_v3_swe_1987_na.nc4...done, 2541980858 bytes.\n",
      "1988\n",
      "Downloading file daymet_v3_swe_1988_na.nc4...done, 2858956966 bytes.\n",
      "1989\n",
      "Downloading file daymet_v3_swe_1989_na.nc4...done, 2881530934 bytes.\n",
      "1990\n",
      "Downloading file daymet_v3_swe_1990_na.nc4...done, 2994050682 bytes.\n",
      "1991\n",
      "Downloading file daymet_v3_swe_1991_na.nc4...done, 2834547393 bytes.\n",
      "1992\n",
      "Downloading file daymet_v3_swe_1992_na.nc4...done, 2810645769 bytes.\n",
      "1993\n",
      "Downloading file daymet_v3_swe_1993_na.nc4...done, 2631051096 bytes.\n",
      "1994\n",
      "Downloading file daymet_v3_swe_1994_na.nc4...done, 2847452960 bytes.\n",
      "1995\n",
      "Downloading file daymet_v3_swe_1995_na.nc4...done, 2838945432 bytes.\n",
      "1996\n",
      "Downloading file daymet_v3_swe_1996_na.nc4...done, 3266315443 bytes.\n",
      "1997\n",
      "Downloading file daymet_v3_swe_1997_na.nc4...done, 2862525089 bytes.\n",
      "1998\n",
      "Downloading file daymet_v3_swe_1998_na.nc4...done, 2659240293 bytes.\n",
      "1999\n",
      "Downloading file daymet_v3_swe_1999_na.nc4...done, 2809645234 bytes.\n",
      "2000\n",
      "Downloading file daymet_v3_swe_2000_na.nc4...done, 2668323564 bytes.\n",
      "2001\n",
      "Downloading file daymet_v3_swe_2001_na.nc4...done, 2618541307 bytes.\n",
      "2002\n",
      "Downloading file daymet_v3_swe_2002_na.nc4...done, 2701740583 bytes.\n",
      "2003\n",
      "Downloading file daymet_v3_swe_2003_na.nc4...done, 2749834872 bytes.\n",
      "2004\n",
      "Downloading file daymet_v3_swe_2004_na.nc4...done, 2845474322 bytes.\n",
      "2005\n",
      "Downloading file daymet_v3_swe_2005_na.nc4...done, 2745810584 bytes.\n",
      "2006\n",
      "Downloading file daymet_v3_swe_2006_na.nc4...done, 2391676277 bytes.\n",
      "2007\n",
      "Downloading file daymet_v3_swe_2007_na.nc4...done, 2678923647 bytes.\n",
      "2008\n",
      "Downloading file daymet_v3_swe_2008_na.nc4...done, 2917101564 bytes.\n",
      "2009\n",
      "Downloading file daymet_v3_swe_2009_na.nc4...done, 2783487480 bytes.\n",
      "2010\n",
      "Downloading file daymet_v3_swe_2010_na.nc4...done, 2508846995 bytes.\n"
     ]
    }
   ],
   "source": [
    "variable = \"swe\"\n",
    "for region in [\"hawaii\", \"puertorico\", \"na\"]:\n",
    "    print(region)\n",
    "    df_region = pd.read_csv(region + '_daymet.csv')\n",
    "    my_basefile = make_filename(variable, 1980, region)\n",
    "    ds_region = xr.open_dataset(my_basefile)\n",
    "    base_region = Region(region, ds_region, df_region, variable)\n",
    "    region_var = base_region.avg\n",
    "    for i in range(1981,2011):\n",
    "        print(i)\n",
    "        my_file = make_filename(variable, i, region)\n",
    "        my_region = Region(region, xr.open_dataset(my_file), base_region.df, variable)\n",
    "        region_var += np.array(my_region.avg)\n",
    "        os.remove(my_file)\n",
    "        del my_region\n",
    "    region_var /= 31.\n",
    "    base_region.df[variable + '_avg'] = region_var\n",
    "    del region_var\n",
    "    base_region.write_daymet()\n",
    "    os.remove(my_basefile)\n",
    "    del base_region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fia_no_pltcn.csv')\n",
    "df_hi = df.loc[df['LAT'] < 25].loc[df['LON'] < -100]\n",
    "df_pr = df.loc[df['LAT'] < 20].loc[df['LON'] > -70]\n",
    "df_na = df[~df.isin(df_hi)][~df.isin(df_pr)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crs = ccrs.LambertConformal(central_longitude=-100, central_latitude=42.5, standard_parallels=(25, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point:\n",
    "    def __init__(self, parent, y, x):\n",
    "        self.parent = parent\n",
    "        self.y = y\n",
    "        self.x = x\n",
    "        self.index = {}\n",
    "        self.original_point = self.parent.ds.sel(y=self.y, method='nearest').sel(x=self.x, method='nearest')\n",
    "        self.find_original_xy_indices()\n",
    "        self.need_recalculate()\n",
    "        self.get_new_point()\n",
    "        \n",
    "    def find_original_xy_indices(self):\n",
    "        self.index['x'] = np.where(self.parent.ds.indexes['x'] == self.original_point.coords['x'].values)[0][0]\n",
    "        self.index['y'] = np.where(self.parent.ds.indexes['y'] == self.original_point.coords['y'].values)[0][0]\n",
    "        \n",
    "    def need_recalculate(self):\n",
    "        if np.isnan(self.original_point['prcp'].isel(time=0)):\n",
    "            self.find_nonnan()\n",
    "        else:\n",
    "            self.indices = self.index['x'], self.index['y']\n",
    "            \n",
    "    def find_nonnan(self):\n",
    "        my_x = self.index['x']\n",
    "        my_y = self.index['y']\n",
    "        for j in [my_y, my_y-1, my_y+1, my_y-2, my_y+2, my_y-3, my_y+3]:\n",
    "            for i in [my_x, my_x-1, my_x+1, my_x-2, my_x+2, my_x-3, my_x+3]:\n",
    "                if i==my_x and j==my_y:\n",
    "                    continue\n",
    "                if i>=self.parent.ds.indexes['x'].size or i<=-1:\n",
    "                    continue\n",
    "                if j>=self.parent.ds.indexes['y'].size or j<=-1:\n",
    "                    continue\n",
    "                if not np.isnan(self.parent.ds['prcp'].isel(time=0).isel(x=i).isel(y=j)):\n",
    "                    self.indices = i, j\n",
    "        self.indices = -1, -1\n",
    "        \n",
    "    def get_new_point(self):\n",
    "        self.point = self.parent.ds.isel(x=self.indices[0]).isel(y=self.indices[1])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_identifier(df):\n",
    "    str_id = df.apply(lambda x: '_'.join(map(str, x)), axis=1)\n",
    "    return pd.factorize(str_id)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!conda install -y -c conda-forge h5netcdf xarray cartopy\n",
    "!{sys.executable} -m pip install azure.storage.blob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
